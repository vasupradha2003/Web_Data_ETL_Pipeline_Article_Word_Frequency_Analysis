{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeiqiSb49qo+A2Pft+750g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasupradha2003/Web_Data_ETL_Pipeline_Article_Word_Frequency_Analysis/blob/main/Web_Data_ETL_Pipeline_Article_Word_Frequency_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFepwenkPUhP"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQEVzT6EQJ-_",
        "outputId": "01770a4a-4efc-444a-e670-d33fc5c531ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Web Scraper class to extract text from the article\n",
        "class WebScraper:\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "\n",
        "    def extract_article_text(self):\n",
        "        response = requests.get(self.url)\n",
        "        html_content = response.content\n",
        "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "        article_text = soup.get_text()\n",
        "        return article_text"
      ],
      "metadata": {
        "id": "r6jkBMCnQQil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Processor class to clean and tokenize the article text\n",
        "class TextProcessor:\n",
        "    def __init__(self, nltk_stopwords):\n",
        "        self.nltk_stopwords = nltk_stopwords\n",
        "\n",
        "    def tokenize_and_clean(self, text):\n",
        "        words = text.split()\n",
        "        filtered_words = [word.lower() for word in words if word.isalpha() and word.lower() not in self.nltk_stopwords]\n",
        "        return filtered_words"
      ],
      "metadata": {
        "id": "LzXcdrR_QQe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ETL Pipeline class that ties everything together\n",
        "class ETLPipeline:\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "        self.nltk_stopwords = set(stopwords.words(\"english\"))\n",
        "\n",
        "    def run(self):\n",
        "        # Step 1: Extract article text\n",
        "        scraper = WebScraper(self.url)\n",
        "        article_text = scraper.extract_article_text()\n",
        "\n",
        "        # Step 2: Process the text (tokenization & cleaning)\n",
        "        processor = TextProcessor(self.nltk_stopwords)\n",
        "        filtered_words = processor.tokenize_and_clean(article_text)\n",
        "\n",
        "        # Step 3: Calculate word frequencies\n",
        "        word_freq = Counter(filtered_words)\n",
        "\n",
        "        # Step 4: Convert the word frequencies to a DataFrame\n",
        "        df = pd.DataFrame(word_freq.items(), columns=[\"Words\", \"Frequencies\"])\n",
        "        df = df.sort_values(by=\"Frequencies\", ascending=False)\n",
        "\n",
        "        return df"
      ],
      "metadata": {
        "id": "bHAMxDXPQQcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function to execute the ETL pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace this with the URL of the new article you want to scrape\n",
        "    article_url = \"https://kahedu.edu.in/why-data-science-is-the-future-of-technology/#:~:text=One%20of%20the%20major%20reasons,realm%20and%20make%20decisions%20swiftly.\"\n",
        "\n",
        "    # Create the ETL pipeline and run it\n",
        "    pipeline = ETLPipeline(article_url)\n",
        "    result_df = pipeline.run()"
      ],
      "metadata": {
        "id": "KlUgAbRLR908"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the top 10 most frequent words\n",
        "print(result_df.head(10))"
      ],
      "metadata": {
        "id": "_pAafB5ISrrh",
        "outputId": "0d5c789c-38e2-4f13-b648-53b54b92f8c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Words  Frequencies\n",
            "0           data           57\n",
            "29    department           50\n",
            "1        science           39\n",
            "102     research           25\n",
            "39   engineering           24\n",
            "3     technology           16\n",
            "4       overview           16\n",
            "75        policy           14\n",
            "2         future           12\n",
            "36      computer           11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AdH0ceTpSuZ2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}